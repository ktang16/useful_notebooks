{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This python script reads in a csv file downloaded from https://nces.ed.gov/collegenavigator/ and searches on Google for parking / transportation for the colleges, finding email contact info\n",
    "\n",
    "- Thank you to https://www.pingshiuanchua.com/blog/post/scraping-search-results-from-google-search for providing helpful code related to web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. Download this Python notebook onto your computer (into folder of your choice)\n",
    "2. Go to https://nces.ed.gov/collegenavigator/ and search for colleges / universities based on criteria (e.g., New York State) and click \"Show Results\"\n",
    "3. Click \"Export Results\" and choose CSV to download the data\n",
    "4. Delete any rows in the CSV file that you don't want contact info for (e.g., colleges with small campus setting). Make sure to delete rows at the end of the file that don't represent info for any colleges (e.g., \"* Cohort Year ...\") and save CSV file in the same folder as this Python notebook\n",
    "5. Edit inputs / constants in the cell below\n",
    "    - Ensure that COLLEGE_DATA_FILE has the name of the CSV file that was downloaded in the previous step\n",
    "    - Note on current settings: code searches for \"parking\" or \"transportation\" department at each college and filters for only edu and org emails\n",
    "    - There's generally no need to edit NUMBER_OF_RESULTS, USER_AGENT, and TIMEOUT_LIMIT\n",
    "6. Run notebook and results will be saved in the same folder as this notebook when web scraping completes\n",
    "7. Contact info is one line per website containing the email in the following format: email(s) (website where email was found);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T22:10:01.963263Z",
     "start_time": "2020-08-17T22:10:01.959517Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Section containing inputs / constants needed for code\n",
    "\"\"\"\n",
    "\n",
    "# File name of csv file downloaded from College Navigator\n",
    "COLLEGE_DATA_FILE = 'CollegeNavigator_Search_2020-08-17_14.11.02.csv'\n",
    "\n",
    "# File name of csv output after web scraping completes\n",
    "OUTPUT_FILE = 'new_york_state_colleges.csv'\n",
    "\n",
    "# Google search text before name of college\n",
    "SEARCH_QUERY = '[parking OR transportation] department '\n",
    "\n",
    "# Regular expression for emails\n",
    "EMAIL_REGEX = r\"[\\w\\.-]+@[\\w-]+\\.edu|[\\w\\.-]+@[\\w-]+\\.org\"\n",
    "\n",
    "# Number of google search results we want to return per search\n",
    "NUMBER_OF_RESULTS = 10\n",
    "\n",
    "# User agent needed to specify it is desktop search results that we want\n",
    "USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0\"\n",
    "\n",
    "# Amount of seconds you are willing to spend opening a website / searching for email addresses\n",
    "TIMEOUT_LIMIT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T22:10:02.464704Z",
     "start_time": "2020-08-17T22:10:01.966423Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "from multiprocessing import Process, Pool, TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for searching Google and finding email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T22:10:02.471793Z",
     "start_time": "2020-08-17T22:10:02.466969Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return all links found after conducting a Google search\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "query: String\n",
    "    Google search query to be conducted\n",
    "NUMBER_OF_RESULTS: int\n",
    "    Number of search results that Google should return for the query\n",
    "USER_AGENT:\n",
    "    User agent for requests library (determines whether Google results are desktop v. mobile)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "links: List\n",
    "    List of all links returned from Google search\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def search_google(query, NUMBER_OF_RESULTS, USER_AGENT):\n",
    "    google_url = \"https://www.google.com/search?q=\" + \\\n",
    "        query + \"&num=\" + str(NUMBER_OF_RESULTS)\n",
    "    google_request = requests.get(google_url, {\"User-Agent\": USER_AGENT})\n",
    "    google_results_soup = BeautifulSoup(google_request.text, \"html.parser\")\n",
    "\n",
    "    result_div = google_results_soup.find_all('div', attrs={'class': 'ZINbbc'})\n",
    "\n",
    "    links = []\n",
    "    for r in result_div:\n",
    "        # Checks if each element is present, else, raise exception\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "\n",
    "            # Check to make sure everything is present before appending\n",
    "            if link != '':\n",
    "                links.append(link['href'])\n",
    "        # Next loop if element is not present\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T22:10:02.477208Z",
     "start_time": "2020-08-17T22:10:02.473408Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleans Google search results from search_google() function to just return websites\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "links: List\n",
    "    List of all links returned from Google search\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "clean_links: List\n",
    "    List of all links returned from Google search (cleaned to only include web addresses)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_google_results(links):\n",
    "    to_remove = []\n",
    "    clean_links = []\n",
    "    for i, l in enumerate(links):\n",
    "        clean = re.search('\\/url\\?q\\=(.*)\\&sa', l)\n",
    "\n",
    "        # Anything that doesn't fit the above pattern will be removed\n",
    "        if clean is None:\n",
    "            to_remove.append(i)\n",
    "            continue\n",
    "        clean_links.append(clean.group(1))\n",
    "\n",
    "    print('  {0} google search result(s) found'.format(len(clean_links)))\n",
    "    return clean_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T22:10:02.482128Z",
     "start_time": "2020-08-17T22:10:02.478686Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts all text elements from a website\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "web_address: String\n",
    "    Website address to extract text from\n",
    "\n",
    "Returns\n",
    "-------\n",
    "website_text: String\n",
    "    All text elements from the website (concatenated together with commas as delimiters)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_text(web_address):\n",
    "    # Download html data from web address\n",
    "    website_html = requests.get(web_address, USER_AGENT)\n",
    "\n",
    "    # Create beautiful soup object and find all text elements\n",
    "    website_soup = BeautifulSoup(website_html.text, \"html.parser\")\n",
    "    website_text_list = website_soup.find_all(text=True)\n",
    "\n",
    "    # Convert list of text elements into string\n",
    "    website_text = ', '.join(website_text_list)\n",
    "\n",
    "    return website_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T22:10:02.486395Z",
     "start_time": "2020-08-17T22:10:02.483622Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts all email addresses from string\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "text: String\n",
    "    Text to search for email addresses\n",
    "\n",
    "Returns\n",
    "-------\n",
    "emails: Set\n",
    "    Set containing all unique email addresses that were found in the text\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_emails(text, EMAIL_REGEX):\n",
    "    emails = set(re.findall(EMAIL_REGEX, text, re.I))\n",
    "\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in CSV from College Navigator and search for parking / transportation emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T22:10:02.517899Z",
     "start_time": "2020-08-17T22:10:02.490009Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in CSV containing college information\n",
    "colleges = pd.read_csv(COLLEGE_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T22:10:02.551450Z",
     "start_time": "2020-08-17T22:10:02.523735Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the google search that needs to be conducted\n",
    "colleges['google_search'] = SEARCH_QUERY + colleges['Name']\n",
    "# Add empty column for contact info\n",
    "colleges['contact_info_found'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T23:51:20.430826Z",
     "start_time": "2020-08-17T22:10:02.553171Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate through all colleges, searching google\n",
    "\n",
    "\n",
    "for index, row in colleges.iterrows():\n",
    "    print(\n",
    "        'Searching on Google for parking / transportation emails for {0}'.format(row['Name']))\n",
    "    \n",
    "    \n",
    "    query = row['google_search']\n",
    "    links = search_google(query, NUMBER_OF_RESULTS, USER_AGENT)\n",
    "    clean_links = clean_google_results(links)\n",
    "    \n",
    "    emails_list = ''\n",
    "    \n",
    "    # Iterate over clean links to find email addresses\n",
    "    for link in clean_links:\n",
    "        # Create a new pool which allows for timeouts\n",
    "        pool1 = Pool(processes=1)\n",
    "        \n",
    "        # Try reading the text from websites\n",
    "        try:\n",
    "            link_text = pool1.apply_async(extract_text, (link,)).get(timeout=TIMEOUT_LIMIT)\n",
    "            pool1.close()\n",
    "            \n",
    "            # Create another pool which allows for timeouts\n",
    "            pool2 = Pool(processes = 1)\n",
    "            \n",
    "            # Try to extract emails from text\n",
    "            try:\n",
    "                email_set = pool2.apply_async(extract_emails, (link_text, EMAIL_REGEX)).get(timeout=TIMEOUT_LIMIT)\n",
    "                pool2.close()\n",
    "            \n",
    "            # If there's a timeout error, just return an empty email set\n",
    "            except TimeoutError:\n",
    "                pool2.terminate()\n",
    "                print('    Took too long to search text for emails in {0}'.format(link))\n",
    "                email_set = {}\n",
    "            \n",
    "            # Calculate how many emails were found\n",
    "            emails_found = len(email_set)\n",
    "            print('    Found {0} email(s) at {1}'.format(emails_found, link))\n",
    "\n",
    "            # If emails were found, add to list\n",
    "            if emails_found > 0:\n",
    "                emails_list += ', '.join(email_set) + ' (' + link + ');\\n'\n",
    "        \n",
    "        # If there are any exceptions, skip and move on to the next link\n",
    "        except:\n",
    "            pool1.terminate()\n",
    "            print('    Took too long / error with loading {0}'.format(link))\n",
    "            continue\n",
    "    \n",
    "    # Add any emails that were found to the dataframe\n",
    "    colleges.at[index, 'contact_info_found'] = emails_list.strip()\n",
    "    print()\n",
    "\n",
    "print('Scraping complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T23:51:20.496524Z",
     "start_time": "2020-08-17T23:51:20.434026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to csv\n"
     ]
    }
   ],
   "source": [
    "# Output results to CSV\n",
    "colleges.to_csv(OUTPUT_FILE, index=False)\n",
    "print('Saved data to csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1069px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
